[2022-01-13 03:02:32,303][root][INFO] - args.local_rank -1
[2022-01-13 03:02:32,303][root][INFO] - WORLD_SIZE None
[2022-01-13 03:02:32,303][root][INFO] - Initialized host 0de619824160 as d.rank -1 on device=cuda, n_gpu=6, world size=1
[2022-01-13 03:02:32,303][root][INFO] - 16-bits training: False 
[2022-01-13 03:02:32,304][root][INFO] - Reading saved model from /root/DPR/outputs/2022-01-11/10-39-38/nq_out/dpr_biencoder.39
[2022-01-13 03:02:36,492][root][INFO] - model_state_dict keys dict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
[2022-01-13 03:02:36,494][root][INFO] - CFG:
[2022-01-13 03:02:36,498][root][INFO] - encoder:
  encoder_model_type: hf_bert
  pretrained_model_cfg: bert-base-uncased
  pretrained_file: null
  projection_dim: 0
  sequence_length: 256
  dropout: 0.1
  fix_ctx_encoder: false
  pretrained: true
ctx_sources:
  dpr_wiki:
    _target_: dpr.data.retriever_data.CsvCtxSrc
    file: data.wikipedia_split.psgs_w100
    id_prefix: 'wiki:'
model_file: /root/DPR/outputs/2022-01-11/10-39-38/nq_out/dpr_biencoder.39
ctx_src: dpr_wiki
encoder_type: ctx
out_file: /data1/jongho/nq/embeddings/dpr_biencoder.39
do_lower_case: true
shard_id: 0
num_shards: 1
batch_size: 1024
tables_as_passages: false
special_tokens: null
tables_chunk_sz: 100
tables_split_type: type1
local_rank: -1
device: cuda
distributed_world_size: 1
distributed_port: null
no_cuda: false
n_gpu: 6
fp16: false
fp16_opt_level: O1

[2022-01-13 03:02:36,503][transformers.file_utils][INFO] - PyTorch version 1.10.1 available.
[2022-01-13 03:02:37,808][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2022-01-13 03:02:37,809][transformers.configuration_utils][INFO] - Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2022-01-13 03:02:37,941][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2022-01-13 03:02:40,637][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing HFBertEncoder.

[2022-01-13 03:02:40,638][transformers.modeling_utils][INFO] - All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
[2022-01-13 03:02:41,482][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
[2022-01-13 03:02:41,483][transformers.configuration_utils][INFO] - Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

[2022-01-13 03:02:41,546][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
[2022-01-13 03:02:44,000][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing HFBertEncoder.

[2022-01-13 03:02:44,000][transformers.modeling_utils][INFO] - All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
[2022-01-13 03:02:44,821][transformers.tokenization_utils_base][INFO] - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
[2022-01-13 03:02:49,194][root][INFO] - Loading saved model state ...
[2022-01-13 03:02:49,380][root][INFO] - reading data source: dpr_wiki
[2022-01-13 03:02:49,383][dpr.data.download_data][INFO] - Requested resource from https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz
[2022-01-13 03:02:49,383][dpr.data.download_data][INFO] - Download root_dir /root/DPR
[2022-01-13 03:02:49,383][dpr.data.download_data][INFO] - File to be downloaded as /root/DPR/downloads/data/wikipedia_split/psgs_w100.tsv
[2022-01-13 03:02:49,383][dpr.data.download_data][INFO] - File already exist /root/DPR/downloads/data/wikipedia_split/psgs_w100.tsv
[2022-01-13 03:06:16,027][root][INFO] - Producing encodings for passages range: 0 to 21015324 (out of total 21015324)
[2022-01-13 03:06:58,077][root][INFO] - Encoded passages 5120
[2022-01-13 03:07:10,647][root][INFO] - Encoded passages 10240
[2022-01-13 03:07:23,268][root][INFO] - Encoded passages 15360
[2022-01-13 03:07:35,917][root][INFO] - Encoded passages 20480
[2022-01-13 03:07:48,473][root][INFO] - Encoded passages 25600
[2022-01-13 03:08:01,111][root][INFO] - Encoded passages 30720
[2022-01-13 03:08:13,793][root][INFO] - Encoded passages 35840
[2022-01-13 03:08:26,482][root][INFO] - Encoded passages 40960
[2022-01-13 03:08:39,134][root][INFO] - Encoded passages 46080
[2022-01-13 03:08:51,717][root][INFO] - Encoded passages 51200
[2022-01-13 03:09:04,305][root][INFO] - Encoded passages 56320
[2022-01-13 03:09:16,828][root][INFO] - Encoded passages 61440
[2022-01-13 03:09:29,498][root][INFO] - Encoded passages 66560
[2022-01-13 03:09:42,105][root][INFO] - Encoded passages 71680
[2022-01-13 03:09:54,704][root][INFO] - Encoded passages 76800
[2022-01-13 03:10:07,314][root][INFO] - Encoded passages 81920
[2022-01-13 03:10:19,835][root][INFO] - Encoded passages 87040
[2022-01-13 03:10:32,454][root][INFO] - Encoded passages 92160
[2022-01-13 03:10:45,080][root][INFO] - Encoded passages 97280
[2022-01-13 03:10:57,635][root][INFO] - Encoded passages 102400
[2022-01-13 03:11:10,163][root][INFO] - Encoded passages 107520
[2022-01-13 03:11:22,686][root][INFO] - Encoded passages 112640
[2022-01-13 03:11:35,246][root][INFO] - Encoded passages 117760
[2022-01-13 03:11:47,816][root][INFO] - Encoded passages 122880
[2022-01-13 03:12:00,391][root][INFO] - Encoded passages 128000
[2022-01-13 03:12:12,979][root][INFO] - Encoded passages 133120
[2022-01-13 03:12:25,538][root][INFO] - Encoded passages 138240
[2022-01-13 03:12:38,057][root][INFO] - Encoded passages 143360
[2022-01-13 03:12:50,652][root][INFO] - Encoded passages 148480
[2022-01-13 03:13:03,235][root][INFO] - Encoded passages 153600
[2022-01-13 03:13:15,797][root][INFO] - Encoded passages 158720
[2022-01-13 03:13:28,401][root][INFO] - Encoded passages 163840
[2022-01-13 03:13:41,141][root][INFO] - Encoded passages 168960
[2022-01-13 03:13:53,823][root][INFO] - Encoded passages 174080
[2022-01-13 03:14:06,428][root][INFO] - Encoded passages 179200
[2022-01-13 03:14:18,932][root][INFO] - Encoded passages 184320
[2022-01-13 03:14:31,566][root][INFO] - Encoded passages 189440
[2022-01-13 03:14:44,159][root][INFO] - Encoded passages 194560
[2022-01-13 03:14:56,781][root][INFO] - Encoded passages 199680
[2022-01-13 03:15:09,380][root][INFO] - Encoded passages 204800
[2022-01-13 03:15:21,905][root][INFO] - Encoded passages 209920
[2022-01-13 03:15:34,366][root][INFO] - Encoded passages 215040
[2022-01-13 03:15:46,803][root][INFO] - Encoded passages 220160
[2022-01-13 03:15:59,352][root][INFO] - Encoded passages 225280
[2022-01-13 03:16:11,894][root][INFO] - Encoded passages 230400
[2022-01-13 03:16:24,561][root][INFO] - Encoded passages 235520
[2022-01-13 03:16:37,158][root][INFO] - Encoded passages 240640
[2022-01-13 03:16:49,671][root][INFO] - Encoded passages 245760
[2022-01-13 03:17:02,205][root][INFO] - Encoded passages 250880
[2022-01-13 03:17:14,874][root][INFO] - Encoded passages 256000
[2022-01-13 03:17:27,455][root][INFO] - Encoded passages 261120
[2022-01-13 03:17:40,000][root][INFO] - Encoded passages 266240
[2022-01-13 03:17:52,574][root][INFO] - Encoded passages 271360
[2022-01-13 03:18:05,129][root][INFO] - Encoded passages 276480
[2022-01-13 03:18:17,688][root][INFO] - Encoded passages 281600
[2022-01-13 03:18:30,270][root][INFO] - Encoded passages 286720
[2022-01-13 03:18:42,841][root][INFO] - Encoded passages 291840
[2022-01-13 03:18:55,705][root][INFO] - Encoded passages 296960
[2022-01-13 03:19:08,446][root][INFO] - Encoded passages 302080
